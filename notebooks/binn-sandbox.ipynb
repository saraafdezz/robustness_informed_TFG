{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloucera/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanpy==1.9.3 anndata==0.10.3 umap==0.5.5 numpy==1.26.1 scipy==1.11.3 pandas==2.1.1 scikit-learn==1.3.1 statsmodels==0.14.0 pynndescent==0.5.11\n",
      "debug=True model_kind='binn-reactome'\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# https://www.sc-best-practices.org/conditions/gsea_pathway.html#id380\n",
    "# Kang HM, Subramaniam M, Targ S, et al. Multiplexed droplet single-cell RNA-sequencing using natural genetic variation\n",
    "#   Nat Biotechnol. 2020 Nov;38(11):1356]. Nat Biotechnol. 2018;36(1):89-94. doi:10.1038/nbt.4042\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from isrobust.bio import (\n",
    "    build_hipathia_renamers,\n",
    "    get_adj_matrices,\n",
    "    get_reactome_adj,\n",
    "    sync_gexp_adj,\n",
    ")\n",
    "from isrobust.datasets import load_kang\n",
    "from isrobust.utils import set_all_seeds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "model_kind = \"binn-reactome\"\n",
    "debug = 1\n",
    "seed = 42\n",
    "model_kind = str(model_kind)\n",
    "debug = bool(int(debug))\n",
    "seed = int(seed)\n",
    "\n",
    "\n",
    "project_path = Path(dotenv.find_dotenv()).parent\n",
    "results_path = project_path.joinpath(\"results\")\n",
    "results_path.mkdir(exist_ok=True, parents=True)\n",
    "data_path = project_path.joinpath(\"data\")\n",
    "data_path.mkdir(exist_ok=True, parents=True)\n",
    "mygene_path = data_path.joinpath(\"mygene\")\n",
    "mygene_path.mkdir(exist_ok=True, parents=True)\n",
    "figs_path = results_path.joinpath(\"figs\")\n",
    "figs_path.mkdir(exist_ok=True, parents=True)\n",
    "tables_path = results_path.joinpath(\"tables\")\n",
    "tables_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "set_all_seeds(seed=seed)\n",
    "\n",
    "sc.set_figure_params(dpi=300, color_map=\"inferno\")\n",
    "sc.settings.verbosity = 1\n",
    "sc.logging.print_header()\n",
    "\n",
    "print(f\"{debug=} {model_kind=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloucera/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "atlas = sc.read(\n",
    "        data_path.joinpath(\"theis\", \"pbmc_vars_sb.h5ad\"),\n",
    "        cache=False,\n",
    "    )\n",
    "\n",
    "atlas = atlas[atlas.obs['study']!='Villani'].copy()\n",
    "atlas.X = atlas.layers[\"counts\"].copy() \n",
    "sc.pp.normalize_total(atlas)\n",
    "sc.pp.log1p(atlas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch</th>\n",
       "      <th>chemistry</th>\n",
       "      <th>data_type</th>\n",
       "      <th>dpt_pseudotime</th>\n",
       "      <th>final_annotation</th>\n",
       "      <th>mt_frac</th>\n",
       "      <th>n_counts</th>\n",
       "      <th>n_genes</th>\n",
       "      <th>sample_ID</th>\n",
       "      <th>size_factors</th>\n",
       "      <th>species</th>\n",
       "      <th>study</th>\n",
       "      <th>tissue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCTGCAGCGAACA-1-Oetjen_A</th>\n",
       "      <td>Oetjen_A</td>\n",
       "      <td>v2_10X</td>\n",
       "      <td>UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD16+ Monocytes</td>\n",
       "      <td>0.047970</td>\n",
       "      <td>6379.0</td>\n",
       "      <td>1862.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957719</td>\n",
       "      <td>Human</td>\n",
       "      <td>Oetjen</td>\n",
       "      <td>Bone_Marrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGCATGTCCTC-1-Oetjen_A</th>\n",
       "      <td>Oetjen_A</td>\n",
       "      <td>v2_10X</td>\n",
       "      <td>UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD4+ T cells</td>\n",
       "      <td>0.024928</td>\n",
       "      <td>4172.0</td>\n",
       "      <td>1082.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>Human</td>\n",
       "      <td>Oetjen</td>\n",
       "      <td>Bone_Marrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGGTCGACTGC-1-Oetjen_A</th>\n",
       "      <td>Oetjen_A</td>\n",
       "      <td>v2_10X</td>\n",
       "      <td>UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD14+ Monocytes</td>\n",
       "      <td>0.051907</td>\n",
       "      <td>6608.0</td>\n",
       "      <td>1618.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.773111</td>\n",
       "      <td>Human</td>\n",
       "      <td>Oetjen</td>\n",
       "      <td>Bone_Marrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGGTCGCTTCT-1-Oetjen_A</th>\n",
       "      <td>Oetjen_A</td>\n",
       "      <td>v2_10X</td>\n",
       "      <td>UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD14+ Monocytes</td>\n",
       "      <td>0.041716</td>\n",
       "      <td>5034.0</td>\n",
       "      <td>1413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.641188</td>\n",
       "      <td>Human</td>\n",
       "      <td>Oetjen</td>\n",
       "      <td>Bone_Marrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGTCCCGACTT-1-Oetjen_A</th>\n",
       "      <td>Oetjen_A</td>\n",
       "      <td>v2_10X</td>\n",
       "      <td>UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NKT cells</td>\n",
       "      <td>0.043522</td>\n",
       "      <td>3998.0</td>\n",
       "      <td>1127.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452426</td>\n",
       "      <td>Human</td>\n",
       "      <td>Oetjen</td>\n",
       "      <td>Bone_Marrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCAAGCTCCTTC-1-Sun_sample4_TC</th>\n",
       "      <td>Sun_sample4_TC</td>\n",
       "      <td>10X</td>\n",
       "      <td>UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD14+ Monocytes</td>\n",
       "      <td>0.059215</td>\n",
       "      <td>3006.0</td>\n",
       "      <td>1111.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.825529</td>\n",
       "      <td>Human</td>\n",
       "      <td>Sun</td>\n",
       "      <td>PBMCs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCAAGCTGAAAT-1-Sun_sample4_TC</th>\n",
       "      <td>Sun_sample4_TC</td>\n",
       "      <td>10X</td>\n",
       "      <td>UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD14+ Monocytes</td>\n",
       "      <td>0.051119</td>\n",
       "      <td>5810.0</td>\n",
       "      <td>1723.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.584353</td>\n",
       "      <td>Human</td>\n",
       "      <td>Sun</td>\n",
       "      <td>PBMCs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCATCATTC-1-Sun_sample4_TC</th>\n",
       "      <td>Sun_sample4_TC</td>\n",
       "      <td>10X</td>\n",
       "      <td>UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NK cells</td>\n",
       "      <td>0.038078</td>\n",
       "      <td>2705.0</td>\n",
       "      <td>1209.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.978014</td>\n",
       "      <td>Human</td>\n",
       "      <td>Sun</td>\n",
       "      <td>PBMCs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCTCGCTTG-1-Sun_sample4_TC</th>\n",
       "      <td>Sun_sample4_TC</td>\n",
       "      <td>10X</td>\n",
       "      <td>UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NK cells</td>\n",
       "      <td>0.052873</td>\n",
       "      <td>2837.0</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.793495</td>\n",
       "      <td>Human</td>\n",
       "      <td>Sun</td>\n",
       "      <td>PBMCs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCTGTCTCG-1-Sun_sample4_TC</th>\n",
       "      <td>Sun_sample4_TC</td>\n",
       "      <td>10X</td>\n",
       "      <td>UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD4+ T cells</td>\n",
       "      <td>0.019406</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>915.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.677142</td>\n",
       "      <td>Human</td>\n",
       "      <td>Sun</td>\n",
       "      <td>PBMCs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32484 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            batch chemistry data_type  \\\n",
       "index                                                                   \n",
       "AAACCTGCAGCGAACA-1-Oetjen_A              Oetjen_A    v2_10X       UMI   \n",
       "AAACCTGCATGTCCTC-1-Oetjen_A              Oetjen_A    v2_10X       UMI   \n",
       "AAACCTGGTCGACTGC-1-Oetjen_A              Oetjen_A    v2_10X       UMI   \n",
       "AAACCTGGTCGCTTCT-1-Oetjen_A              Oetjen_A    v2_10X       UMI   \n",
       "AAACCTGTCCCGACTT-1-Oetjen_A              Oetjen_A    v2_10X       UMI   \n",
       "...                                           ...       ...       ...   \n",
       "TTTGTCAAGCTCCTTC-1-Sun_sample4_TC  Sun_sample4_TC       10X       UMI   \n",
       "TTTGTCAAGCTGAAAT-1-Sun_sample4_TC  Sun_sample4_TC       10X       UMI   \n",
       "TTTGTCATCATCATTC-1-Sun_sample4_TC  Sun_sample4_TC       10X       UMI   \n",
       "TTTGTCATCTCGCTTG-1-Sun_sample4_TC  Sun_sample4_TC       10X       UMI   \n",
       "TTTGTCATCTGTCTCG-1-Sun_sample4_TC  Sun_sample4_TC       10X       UMI   \n",
       "\n",
       "                                   dpt_pseudotime final_annotation   mt_frac  \\\n",
       "index                                                                          \n",
       "AAACCTGCAGCGAACA-1-Oetjen_A                   NaN  CD16+ Monocytes  0.047970   \n",
       "AAACCTGCATGTCCTC-1-Oetjen_A                   NaN     CD4+ T cells  0.024928   \n",
       "AAACCTGGTCGACTGC-1-Oetjen_A                   NaN  CD14+ Monocytes  0.051907   \n",
       "AAACCTGGTCGCTTCT-1-Oetjen_A                   NaN  CD14+ Monocytes  0.041716   \n",
       "AAACCTGTCCCGACTT-1-Oetjen_A                   NaN        NKT cells  0.043522   \n",
       "...                                           ...              ...       ...   \n",
       "TTTGTCAAGCTCCTTC-1-Sun_sample4_TC             NaN  CD14+ Monocytes  0.059215   \n",
       "TTTGTCAAGCTGAAAT-1-Sun_sample4_TC             NaN  CD14+ Monocytes  0.051119   \n",
       "TTTGTCATCATCATTC-1-Sun_sample4_TC             NaN         NK cells  0.038078   \n",
       "TTTGTCATCTCGCTTG-1-Sun_sample4_TC             NaN         NK cells  0.052873   \n",
       "TTTGTCATCTGTCTCG-1-Sun_sample4_TC             NaN     CD4+ T cells  0.019406   \n",
       "\n",
       "                                   n_counts  n_genes sample_ID  size_factors  \\\n",
       "index                                                                          \n",
       "AAACCTGCAGCGAACA-1-Oetjen_A          6379.0   1862.0         0      0.957719   \n",
       "AAACCTGCATGTCCTC-1-Oetjen_A          4172.0   1082.0         0      0.425532   \n",
       "AAACCTGGTCGACTGC-1-Oetjen_A          6608.0   1618.0         0      0.773111   \n",
       "AAACCTGGTCGCTTCT-1-Oetjen_A          5034.0   1413.0         0      0.641188   \n",
       "AAACCTGTCCCGACTT-1-Oetjen_A          3998.0   1127.0         0      0.452426   \n",
       "...                                     ...      ...       ...           ...   \n",
       "TTTGTCAAGCTCCTTC-1-Sun_sample4_TC    3006.0   1111.0         3      0.825529   \n",
       "TTTGTCAAGCTGAAAT-1-Sun_sample4_TC    5810.0   1723.0         3      1.584353   \n",
       "TTTGTCATCATCATTC-1-Sun_sample4_TC    2705.0   1209.0         3      0.978014   \n",
       "TTTGTCATCTCGCTTG-1-Sun_sample4_TC    2837.0   1045.0         3      0.793495   \n",
       "TTTGTCATCTGTCTCG-1-Sun_sample4_TC    3504.0    915.0         3      0.677142   \n",
       "\n",
       "                                  species   study       tissue  \n",
       "index                                                           \n",
       "AAACCTGCAGCGAACA-1-Oetjen_A         Human  Oetjen  Bone_Marrow  \n",
       "AAACCTGCATGTCCTC-1-Oetjen_A         Human  Oetjen  Bone_Marrow  \n",
       "AAACCTGGTCGACTGC-1-Oetjen_A         Human  Oetjen  Bone_Marrow  \n",
       "AAACCTGGTCGCTTCT-1-Oetjen_A         Human  Oetjen  Bone_Marrow  \n",
       "AAACCTGTCCCGACTT-1-Oetjen_A         Human  Oetjen  Bone_Marrow  \n",
       "...                                   ...     ...          ...  \n",
       "TTTGTCAAGCTCCTTC-1-Sun_sample4_TC   Human     Sun        PBMCs  \n",
       "TTTGTCAAGCTGAAAT-1-Sun_sample4_TC   Human     Sun        PBMCs  \n",
       "TTTGTCATCATCATTC-1-Sun_sample4_TC   Human     Sun        PBMCs  \n",
       "TTTGTCATCTCGCTTG-1-Sun_sample4_TC   Human     Sun        PBMCs  \n",
       "TTTGTCATCTGTCTCG-1-Sun_sample4_TC   Human     Sun        PBMCs  \n",
       "\n",
       "[32484 rows x 13 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atlas.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "AAACATACATTTCC.1    CD14 Mono\n",
       "AAACATACCAGAAA.1    CD14 Mono\n",
       "AAACATACCTCGCT.1    CD14 Mono\n",
       "AAACATACGATGAA.1        CD4 T\n",
       "AAACATACGGCATT.1    CD14 Mono\n",
       "                      ...    \n",
       "TTTGCATGAACGAA.1           DC\n",
       "TTTGCATGACGTAC.1        CD4 T\n",
       "TTTGCATGCCTGTC.1            B\n",
       "TTTGCATGCTAAGC.1        CD4 T\n",
       "TTTGCATGGGACGA.1        CD4 T\n",
       "Name: cell_type, Length: 13576, dtype: category\n",
       "Categories (8, object): ['B', 'CD4 T', 'CD8 T', 'CD14 Mono', 'CD16 Mono', 'DC', 'NK', 'T']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs.cell_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloucera/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "adata = load_kang(data_folder=data_path, normalize=True, n_genes=4000)\n",
    "\n",
    "# %%\n",
    "x_trans = adata.to_df()\n",
    "\n",
    "obs = adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "circuit_adj, circuit_to_pathway_adj = get_adj_matrices(\n",
    "    gene_list=x_trans.columns.to_list()\n",
    ")\n",
    "\n",
    "circuit_renamer, pathway_renamer, circuit_to_effector = build_hipathia_renamers()\n",
    "\n",
    "kegg_circuit_names = circuit_adj.rename(columns=circuit_renamer).columns\n",
    "\n",
    "kegg_pathway_names = circuit_to_pathway_adj.rename(columns=pathway_renamer).columns\n",
    "\n",
    "circuit_adj.head()\n",
    "\n",
    "# %%\n",
    "reactome = get_reactome_adj()\n",
    "reactome_pathway_names = reactome.columns\n",
    "\n",
    "x_trans, circuit_adj = sync_gexp_adj(gexp=x_trans, adj=circuit_adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def train_val_test_split(features, val_size, test_size, stratify, seed):\n",
    "    train_size = 1 - (val_size + test_size)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        features,\n",
    "        stratify,\n",
    "        train_size=train_size,\n",
    "        stratify=stratify,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    x_val, x_test, y_val, y_test = train_test_split(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        test_size=test_size / (test_size + val_size),\n",
    "        stratify=y_test,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    x_train = x_train.astype(\"float32\")\n",
    "    x_val = x_val.astype(\"float32\")\n",
    "    x_test = x_test.astype(\"float32\")\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    x_trans.apply(minmax_scale),\n",
    "    val_size=0.20,\n",
    "    test_size=0.20,\n",
    "    stratify=obs[\"cell_type\"].astype(str) + obs[\"condition\"].astype(str),\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "y_train = obs[\"cell_type\"][y_train.index]\n",
    "y_val = obs[\"cell_type\"][y_val.index]\n",
    "y_test = obs[\"cell_type\"][y_test.index]\n",
    "\n",
    "label_encoder = OneHotEncoder(sparse_output=False).fit(y_train.to_frame())\n",
    "y_train_encoded = label_encoder.transform(y_train.to_frame())\n",
    "y_val_encoded = label_encoder.transform(y_val.to_frame())\n",
    "y_test_encoded = label_encoder.transform(y_test.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binn import BINN, Network\n",
    "\n",
    "input_data = pd.read_csv(\"../../BINN/data/test_qm.csv\", sep=\",\")\n",
    "translation = pd.read_csv(\"../../BINN/data/translation.tsv\", sep=\"\\t\")\n",
    "pathways = pd.read_csv(\"../../BINN/data/pathways.tsv\", sep=\"\\t\").rename(columns={\"parent\": \"source\", \"child\": \"target\"})\n",
    "\n",
    "network = Network(\n",
    "    input_data=input_data,\n",
    "    pathways=pathways,\n",
    "    mapping=translation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PurePath.as_posix of PosixPath('/home/cloucera/github/robustness_informed/data/mygene')>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mygene_path.as_posix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:biothings.client:[ Future queries will be cached in \"/home/cloucera/github/robustness_informed/data/mygene.sqlite\" ]\n",
      "INFO:biothings.client:querying 1-1000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 1001-2000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 2001-3000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 3001-4000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 4001-5000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 5001-6000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 6001-7000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 7001-8000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 8001-9000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 9001-10000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 10001-11000...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:querying 11001-11613...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:Finished.\n",
      "WARNING:biothings.client:87 input query terms found dup hits:\t[('O14950', 2), ('O15263', 2), ('O43508', 2), ('O75022', 2), ('O95925', 2), ('P01562', 2), ('P04001'\n",
      "WARNING:biothings.client:909 input query terms found no hit:\t['A0A075B6P5', 'A0A075B6S6', 'A0A0A6YYK7', 'A0A0C4DH25', 'A0A0C4DH73', 'A0M8Q6', 'A2KUC3', 'A2NJV5',\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'out':            notfound     _id     _score   symbol\n",
       " query                                          \n",
       " A0A075B6P5     True     NaN        NaN      NaN\n",
       " A0A075B6S6     True     NaN        NaN      NaN\n",
       " A0A096LP49      NaN  399693  19.768894  CCDC187\n",
       " A0A0A6YYK7     True     NaN        NaN      NaN\n",
       " A0A0C4DH25     True     NaN        NaN      NaN\n",
       " ...             ...     ...        ...      ...\n",
       " Q9Y6X5          NaN   22875  19.768894    ENPP4\n",
       " Q9Y6X9          NaN   22880  19.768894    MORC2\n",
       " Q9Y6Y8          NaN   11196  19.768894  SEC23IP\n",
       " Q9Y6Y9          NaN   23643  19.768894     LY96\n",
       " Q9Y6Z7          NaN   10584  19.768894  COLEC10\n",
       " \n",
       " [11737 rows x 4 columns],\n",
       " 'dup':      query  duplicate hits\n",
       " 0   O14950               2\n",
       " 1   O15263               2\n",
       " 2   O43508               2\n",
       " 3   O75022               2\n",
       " 4   O95925               2\n",
       " ..     ...             ...\n",
       " 82  Q9ULR0               2\n",
       " 83  Q9Y3B3               2\n",
       " 84  Q9Y3E7               2\n",
       " 85  Q9Y575               2\n",
       " 86  Q9Y6Q2               2\n",
       " \n",
       " [87 rows x 2 columns],\n",
       " 'missing':           query\n",
       " 0    A0A075B6P5\n",
       " 1    A0A075B6S6\n",
       " 2    A0A0A6YYK7\n",
       " 3    A0A0C4DH25\n",
       " 4    A0A0C4DH73\n",
       " ..          ...\n",
       " 904    Q9Y6H5-2\n",
       " 905      Q9Y6N3\n",
       " 906    Q9Y6P5-1\n",
       " 907    Q9Y6P5-3\n",
       " 908    Q9Y6W8-1\n",
       " \n",
       " [909 rows x 1 columns]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRANSLATE_GENES = True\n",
    "\n",
    "if TRANSLATE_GENES:\n",
    "    import mygene\n",
    "\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    mg.set_caching(mygene_path.as_posix())\n",
    "    gene_trans_df = mg.querymany(translation.input.unique(), scopes=\"uniprot\", fields=\"symbol\", as_dataframe=True, species='human', returnall=True)\n",
    "\n",
    "\n",
    "gene_trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>R-HSA-9013106</td>\n",
       "      <td>CCDC187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>R-HSA-6809371</td>\n",
       "      <td>LCE6A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>R-HSA-5620924</td>\n",
       "      <td>IFT56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>R-HSA-5620924</td>\n",
       "      <td>IFT56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>R-HSA-8866654</td>\n",
       "      <td>TMEM129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51360</th>\n",
       "      <td>R-HSA-937072</td>\n",
       "      <td>LY96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51361</th>\n",
       "      <td>R-HSA-9707616</td>\n",
       "      <td>LY96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51362</th>\n",
       "      <td>R-HSA-975163</td>\n",
       "      <td>LY96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51363</th>\n",
       "      <td>R-HSA-166662</td>\n",
       "      <td>COLEC10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51364</th>\n",
       "      <td>R-HSA-166663</td>\n",
       "      <td>COLEC10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45963 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         translation    input\n",
       "52     R-HSA-9013106  CCDC187\n",
       "111    R-HSA-6809371    LCE6A\n",
       "112    R-HSA-5620924    IFT56\n",
       "113    R-HSA-5620924    IFT56\n",
       "114    R-HSA-8866654  TMEM129\n",
       "...              ...      ...\n",
       "51360   R-HSA-937072     LY96\n",
       "51361  R-HSA-9707616     LY96\n",
       "51362   R-HSA-975163     LY96\n",
       "51363   R-HSA-166662  COLEC10\n",
       "51364   R-HSA-166663  COLEC10\n",
       "\n",
       "[45963 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation = translation.merge(gene_trans_df[\"out\"].dropna(subset=\"symbol\").reset_index(names=\"input\")[[\"input\", \"symbol\"]],\n",
    "                  how=\"left\",\n",
    "                  ).dropna(subset=\"symbol\").drop([\"input\", \"Unnamed: 0\"], axis=1).rename(columns={\"symbol\": \"input\"})\n",
    "\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = x_trans.columns.intersection(translation.input)\n",
    "\n",
    "translation = translation.loc[translation.input.isin(genes)]\n",
    "x_trans = x_trans.loc[:, genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>PPARD</td>\n",
       "      <td>P-hsa03320-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>RXRA</td>\n",
       "      <td>P-hsa03320-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>RXRB</td>\n",
       "      <td>P-hsa03320-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>UBC</td>\n",
       "      <td>P-hsa03320-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>PDPK1</td>\n",
       "      <td>P-hsa03320-45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source         target\n",
       "1063  PPARD  P-hsa03320-62\n",
       "1267   RXRA  P-hsa03320-62\n",
       "1268   RXRB  P-hsa03320-62\n",
       "1454    UBC  P-hsa03320-62\n",
       "2801  PDPK1  P-hsa03320-45"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kegg_trans = circuit_adj.melt(ignore_index=False).reset_index(names=\"symbol\").query(\"value>0\").rename(columns={\"symbol\": \"source\", \"circuit\": \"target\"}).drop(\"value\", axis=1)\n",
    "\n",
    "kegg_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network(\n",
    "    input_data=x_trans.T.reset_index(names=\"gene\"),\n",
    "    pathways=pathways,\n",
    "    input_data_column=\"gene\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    x_trans.apply(minmax_scale),\n",
    "    val_size=0.20,\n",
    "    test_size=0.20,\n",
    "    stratify=obs[\"cell_type\"].astype(str) + obs[\"condition\"].astype(str),\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "y_train = obs[\"cell_type\"][y_train.index]\n",
    "y_val = obs[\"cell_type\"][y_val.index]\n",
    "y_test = obs[\"cell_type\"][y_test.index]\n",
    "\n",
    "label_encoder = OneHotEncoder(sparse_output=False).fit(y_train.to_frame())\n",
    "y_train_encoded = label_encoder.transform(y_train.to_frame())\n",
    "y_val_encoded = label_encoder.transform(y_val.to_frame())\n",
    "y_test_encoded = label_encoder.transform(y_test.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "\n",
    "callback = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Stop training when `val_loss` is no longer improving\n",
    "    min_delta=1e-1,  # \"no longer improving\" being defined as \"no better than 1e-5 less\"\n",
    "    patience=100,  # \"no longer improving\" being further defined as \"for at least 3 epochs\"\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "vae, encoder, decoder = build_kegg_vae(\n",
    "        circuits=circuit_adj, pathways=circuit_to_pathway_adj, seed=seed\n",
    "    )\n",
    "\n",
    "history = vae.fit(\n",
    "    x_train.values,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    epochs=N_EPOCHS,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[callback],\n",
    "    validation_data=(x_val.values, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network(\n",
    "    input_data=x_trans.T.reset_index(names=\"gene\"),\n",
    "    pathways=pathways,\n",
    "    mapping=translation,\n",
    "    input_data_column=\"gene\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BINN is on the device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BINN(\n",
       "  (layers): Sequential(\n",
       "    (Layer_0): Linear(in_features=917, out_features=172, bias=True)\n",
       "    (BatchNorm_0): BatchNorm1d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (Dropout_0): Dropout(p=0, inplace=False)\n",
       "    (Tanh 0): Tanh()\n",
       "    (Layer_1): Linear(in_features=172, out_features=542, bias=True)\n",
       "    (BatchNorm_1): BatchNorm1d(542, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (Dropout_1): Dropout(p=0, inplace=False)\n",
       "    (Tanh 1): Tanh()\n",
       "    (Output layer): Linear(in_features=542, out_features=8, bias=True)\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from binn import BINNClassifier, BINN\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "binn = BINN(\n",
    "    n_layers=2,\n",
    "    network=network,\n",
    "    n_outputs=y_train_encoded.shape[1],\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "binn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "n_workers = os.cpu_count() - 1\n",
    "n_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "            dataset=TensorDataset(torch.Tensor(x_train.values), torch.Tensor(y_train_encoded)),\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            num_workers=n_workers\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14803, 1650)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 3 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=3)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name   | Type             | Params\n",
      "--------------------------------------------\n",
      "0 | layers | Sequential       | 257 K \n",
      "1 | loss   | CrossEntropyLoss | 0     \n",
      "--------------------------------------------\n",
      "257 K     Trainable params\n",
      "0         Non-trainable params\n",
      "257 K     Total params\n",
      "1.030     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/463 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x1650 and 917x172)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/cloucera/github/robustness_informed/notebooks/binn-sandbox.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-80g/home/cloucera/github/robustness_informed/notebooks/binn-sandbox.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-80g/home/cloucera/github/robustness_informed/notebooks/binn-sandbox.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-80g/home/cloucera/github/robustness_informed/notebooks/binn-sandbox.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-80g/home/cloucera/github/robustness_informed/notebooks/binn-sandbox.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu-80g/home/cloucera/github/robustness_informed/notebooks/binn-sandbox.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(binn, dataloader)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1036\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    137\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], batch_idx, kwargs)\n\u001b[1;32m    241\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(batch_idx, closure)\n\u001b[1;32m    189\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    264\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    266\u001b[0m     trainer,\n\u001b[1;32m    267\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    268\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    269\u001b[0m     batch_idx,\n\u001b[1;32m    270\u001b[0m     optimizer,\n\u001b[1;32m    271\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    275\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1291\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1257\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1258\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1259\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \n\u001b[1;32m   1290\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 230\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(optimizer, model\u001b[39m=\u001b[39;49mmodel, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 143\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    145\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    146\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/binn/binn.py:145\u001b[0m, in \u001b[0;36mBINN.training_step\u001b[0;34m(self, batch, _)\u001b[0m\n\u001b[1;32m    143\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    144\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 145\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    146\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(y_hat, y)\n\u001b[1;32m    147\u001b[0m prediction \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(y_hat, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/binn/binn.py:129\u001b[0m, in \u001b[0;36mBINN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_residual(x)\n\u001b[1;32m    128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/github/robustness_informed/.venvs/binn/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x1650 and 917x172)"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[],\n",
    "    max_epochs=10,\n",
    ")\n",
    "\n",
    "trainer.fit(binn, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_importances(data, abs=False):\n",
    "    if abs:\n",
    "        return np.abs(data).mean(axis=0)\n",
    " v   else:\n",
    "        return data.mean(axis=0)\n",
    "\n",
    "\n",
    "def get_actiations(act_model, layer_id, data):\n",
    "    data_encoded = act_model.predict(data)[layer_id]\n",
    "    return data_encoded\n",
    "\n",
    "\n",
    "# %%\n",
    "def train_val_test_split(features, val_size, test_size, stratify, seed):\n",
    "    train_size = 1 - (val_size + test_size)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        features,\n",
    "        stratify,\n",
    "        train_size=train_size,\n",
    "        stratify=stratify,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    x_val, x_test = train_test_split(\n",
    "        x_test,\n",
    "        test_size=test_size / (test_size + val_size),\n",
    "        stratify=y_test,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    x_train = x_train.astype(\"float32\")\n",
    "    x_val = x_val.astype(\"float32\")\n",
    "    x_test = x_test.astype(\"float32\")\n",
    "\n",
    "    return x_train, x_val, x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "results_path_model = results_path.joinpath(model_kind)\n",
    "obs = adata.obs.copy()\n",
    "results_path_model.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "x_train, x_val, x_test = train_val_test_split(\n",
    "    x_trans.apply(minmax_scale),\n",
    "    val_size=0.20,\n",
    "    test_size=0.20,\n",
    "    stratify=obs[\"cell_type\"].astype(str) + obs[\"condition\"].astype(str),\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "if model_kind == \"ivae_kegg\":\n",
    "    vae, encoder, decoder = build_kegg_vae(\n",
    "        circuits=circuit_adj, pathways=circuit_to_pathway_adj, seed=seed\n",
    "    )\n",
    "elif model_kind == \"ivae_reactome\":\n",
    "    vae, encoder, decoder = build_reactome_vae(reactome, seed=seed)\n",
    "else:\n",
    "    raise NotImplementedError(\"Model not yet implemented.\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "callback = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Stop training when `val_loss` is no longer improving\n",
    "    min_delta=1e-1,  # \"no longer improving\" being defined as \"no better than 1e-5 less\"\n",
    "    patience=100,  # \"no longer improving\" being further defined as \"for at least 3 epochs\"\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "history = vae.fit(\n",
    "    x_train.values,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    epochs=N_EPOCHS,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[callback],\n",
    "    validation_data=(x_val.values, None),\n",
    ")\n",
    "\n",
    "evaluation = {}\n",
    "evaluation[\"train\"] = vae.evaluate(\n",
    "    x_train, vae.predict(x_train), verbose=0, return_dict=True\n",
    ")\n",
    "evaluation[\"val\"] = vae.evaluate(x_val, vae.predict(x_val), verbose=0, return_dict=True)\n",
    "evaluation[\"test\"] = vae.evaluate(\n",
    "    x_test, vae.predict(x_test), verbose=0, return_dict=True\n",
    ")\n",
    "\n",
    "pd.DataFrame.from_dict(evaluation).reset_index(names=\"metric\").assign(seed=seed).melt(\n",
    "    id_vars=[\"seed\", \"metric\"],\n",
    "    value_vars=[\"train\", \"val\", \"test\"],\n",
    "    var_name=\"split\",\n",
    "    value_name=\"score\",\n",
    ").assign(model=model_kind).to_pickle(\n",
    "    results_path_model.joinpath(f\"metrics-seed-{seed:02d}.pkl\")\n",
    ")\n",
    "\n",
    "layer_outputs = [layer.output for layer in encoder.layers]\n",
    "activation_model = Model(inputs=encoder.input, outputs=layer_outputs)\n",
    "\n",
    "# only analyze informed and funnel layers\n",
    "for layer_id in range(1, len(layer_outputs)):\n",
    "    if model_kind == \"ivae_kegg\":\n",
    "        if layer_id == 1:\n",
    "            colnames = kegg_circuit_names\n",
    "            layer_name = \"circuits\"\n",
    "        elif layer_id == 2:\n",
    "            colnames = kegg_pathway_names\n",
    "            layer_name = \"pathways\"\n",
    "        elif layer_id == (len(layer_outputs) - 1):\n",
    "            n_latents = len(kegg_pathway_names) // 2\n",
    "            colnames = [f\"latent_{i:02d}\" for i in range(n_latents)]\n",
    "            layer_name = \"funnel\"\n",
    "        else:\n",
    "            continue\n",
    "    elif model_kind == \"ivae_reactome\":\n",
    "        if layer_id == 1:\n",
    "            colnames = reactome_pathway_names\n",
    "            layer_name = \"pathways\"\n",
    "        elif layer_id == (len(layer_outputs) - 1):\n",
    "            n_latents = len(reactome_pathway_names) // 2\n",
    "            colnames = [f\"latent_{i:02d}\" for i in range(n_latents)]\n",
    "            layer_name == \"funnel\"\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        raise NotImplementedError(\"Model not yet implemented.\")\n",
    "\n",
    "    print(f\"encoding layer {layer_id}\")\n",
    "\n",
    "    encodings = get_activations(\n",
    "        act_model=activation_model,\n",
    "        layer_id=layer_id,\n",
    "        data=x_trans.apply(minmax_scale),\n",
    "    )\n",
    "    encodings = pd.DataFrame(encodings, index=x_trans.index, columns=colnames)\n",
    "    encodings[\"split\"] = \"train\"\n",
    "    encodings.loc[x_val.index, \"split\"] = \"val\"\n",
    "    encodings.loc[x_test.index, \"split\"] = \"test\"\n",
    "    encodings[\"layer\"] = layer_name\n",
    "    encodings[\"seed\"] = seed\n",
    "    encodings[\"model\"] = model_kind\n",
    "    encodings = encodings.merge(\n",
    "        obs[[\"cell_type\", \"condition\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    encodings.to_pickle(\n",
    "        results_path_model.joinpath(\n",
    "            f\"encodings_layer-{layer_id:02d}_seed-{seed:02d}.pkl\"\n",
    "        )\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
